# Machine_Learning_Coursera_pythonversion
This repository is based on Coursera's course "<strong>Machine learning</strong>" by Prof. Andrew Ng from Stanford University (https://www.coursera.org/learn/machine-learning). In this course, all codes for the exercises should be written in Octave or Matlab. In addition, some codes are partially provided in advance.

Here, I wrote all codes using Python 3 by myself. This repository can be regarded as my study notebook because I added some explanation and did additional analysis.  
** Note: Some equations cannot be read on Github. However, they are visible on jupyter notebook environment.

## [Ex1](https://github.com/ksonod/Machine_Learning_Coursera_pythonversion/blob/master/Ex1/Ex1.ipynb)
- Linear regression with one and multiple variables
- Gradient descent
- Hypothesis h
- Cost function J
- Learning rate
- Feature normalization
- Normal equation  
<img src="https://i.imgur.com/kfPOYIl.png" width="600px">  

## [Ex2](https://github.com/ksonod/Machine_Learning_Coursera_pythonversion/blob/master/Ex2/Ex2.ipynb)  
- Logistic regression
- Regularized logistic regression
- Feature mapping
- Decision boundary    
<img src="https://i.imgur.com/dulLjZ3.png" width="500px">    

## [Ex3](https://github.com/ksonod/Machine_Learning_Coursera_pythonversion/blob/master/Ex3/Ex3.ipynb)  
- Multi-class classification
- Regularized logistic regression
- One-vs-all logistic regression
- Neural network
- Handwriting recognition  
<img src="https://i.imgur.com/eBaANmC.png" width="300px">     

## [Ex4](https://github.com/ksonod/Machine_Learning_Coursera_pythonversion/blob/master/Ex4/Ex4.ipynb)
- Handwriting recognition
- Neural network
- Regularized cost function and gradient for the neural network
- Feedforward computation
- Backpropagation algorithm
- Gradient checking
<img src="https://i.imgur.com/jCye6P1.png" width="900px">       

## [Ex5](https://github.com/ksonod/Machine_Learning_Coursera_pythonversion/blob/master/Ex5/Ex5.ipynb)
- Cross validation
- Regularized linear regression
- Cost function
- Hypothesis
- Bias-variance
- Learning curves
- Polynomial regression
- Fearture normalization  
<img src="https://i.imgur.com/T6gkLD3.png" width="500px">    
  
## [Ex6](https://github.com/ksonod/Machine_Learning_Coursera_pythonversion/blob/master/Ex6/Ex6.ipynb)
- Support vector machine
- Decision boundary
- Linear classification
- C parameter
- Gaussian kernels
- Spam classification
<img src="https://i.imgur.com/cbhDn9T.png" width="500px">    
  

## [Ex7](https://github.com/ksonod/Machine_Learning_Coursera_pythonversion/blob/master/Ex7/Ex7.ipynb)
- K-means clustering
- Random initialization
- Image compression
- Principal component analysis (PCA)
- Dimensionality reduction
- Covariance matrix
- Singular value decomposition (SVD)   
<img src="https://i.imgur.com/vUb2Xvw.png" width="500px">    
